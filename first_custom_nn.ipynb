{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n",
    "# The ' (single quote) is for the words from the unix dictionary 'words'\n",
    "ALL_LETTERS = string.ascii_letters + \"'\"\n",
    "LETTERS_TOTAL = len(ALL_LETTERS) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LETTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTERS_TOTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/usr/share/dict/words') as fo:\n",
    "    words = fo.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    '''Return \"normalised\" string for ASCII format'''\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in ALL_LETTERS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [unicodeToAscii(word) for word in words if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of empty strings: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"number of empty strings:\", len([w for w in words if len(w) == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class firstRNN(nn.Module):\n",
    "    '''My first ever Recurrent Neural Network'''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(output_size + hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        '''Forward function for training'''\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        combined_output = torch.cat((output, hidden), 1)\n",
    "        output = self.o2o(combined_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "        \n",
    "    def getInitialisedHidden(self):\n",
    "        return torch.ones([1, self.hidden_size])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomWord(l):\n",
    "    '''Return a random item from a list'''\n",
    "    return random.choice(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomWord(['cmon', 'fuck', 'what', 'is', 'this', '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputTensor(word):\n",
    "    '''Return a tensor representing a word in terms of multi-dimensional ndarrays of 0s and 1s'''\n",
    "    # Each letter is represented as an array of 0s and 1 represents index of the letter.\n",
    "    # Keep in mind that the '1' in the function creates a tensor to contain the array,\n",
    "    # because pytorch accepts input as batchers rather than 'actual' values.\n",
    "    tensor = torch.zeros(len(word), 1, LETTERS_TOTAL)\n",
    "    for letter_tensor, letter in zip(tensor, word):\n",
    "        letter_tensor[0][ALL_LETTERS.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def targetTensor(word):\n",
    "    '''Return a tensor of type Long that represents letters in terms of index positions including EOS'''\n",
    "    return torch.LongTensor([ALL_LETTERS.find(letter) for letter in word] + [LETTERS_TOTAL - 1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS FOR inputTensor FUNCTION\n",
    "assert inputTensor('a').shape == (1, 1, LETTERS_TOTAL)\n",
    "t = torch.zeros(1, 1, LETTERS_TOTAL)\n",
    "t[0][0][0] = 1\n",
    "assert torch.all(torch.eq(t, inputTensor('a')))\n",
    "\n",
    "assert inputTensor(\"a'\").shape == (2, 1, LETTERS_TOTAL)\n",
    "t = torch.zeros(2, 1, LETTERS_TOTAL)\n",
    "t[0][0][0] = 1\n",
    "t[1][0][LETTERS_TOTAL - 2] = 1 # -2 coz the -1 is EOS\n",
    "assert torch.all(torch.eq(t, inputTensor(\"a'\")))\n",
    "\n",
    "assert inputTensor(\"az'\").shape == (3, 1, LETTERS_TOTAL)\n",
    "t = torch.zeros(3, 1, LETTERS_TOTAL)\n",
    "t[0][0][0] = 1\n",
    "t[1][0][25] = 1 # Index for letter z\n",
    "t[2][0][LETTERS_TOTAL - 2] = 1 # -2 coz the -1 is EOS\n",
    "assert torch.all(torch.eq(t, inputTensor(\"az'\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS FOR targetTensor FUNCTION\n",
    "assert list(targetTensor('a').shape) == [2]\n",
    "assert torch.all(torch.eq(targetTensor('a'), torch.LongTensor([0, 53])))\n",
    "\n",
    "assert list(targetTensor(\"a'\").shape) == [3]\n",
    "assert torch.all(torch.eq(targetTensor(\"a'\"), torch.LongTensor([0, 52, 53])))\n",
    "\n",
    "assert list(targetTensor(\"az'\").shape) == [4]\n",
    "assert torch.all(torch.eq(targetTensor(\"az'\"), torch.LongTensor([0, 25, 52, 53])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def getRandomTraningTensorSet(words):\n",
    "    '''Return an input tensor and a target tensor for a randomly selected word'''\n",
    "    random_word = random.choice(words)\n",
    "    return inputTensor(random_word), targetTensor(random_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for getRandomTrainingTensorSet function\n",
    "in_t, out_t = getRandomTraningTensorSet(['a'])\n",
    "exp_in_t, exp_out_t = torch.Tensor([[[1]+53*[0]]]), torch.LongTensor([0, 53])\n",
    "assert torch.all(torch.eq(exp_in_t, in_t))\n",
    "assert torch.all(torch.eq(exp_out_t, out_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranining time!\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def train(input_word_tensor, target_word_tensor, rnn):\n",
    "    '''Train a given RNN model with an input and target tensors that represent a word'''\n",
    "    # unsqueeze function makes each value in 1d tensor to be placed within\n",
    "    # its own tensor. (e.g. tensor of shape (4) turns into shape (4, 1))\n",
    "    target_word_tensor.unsqueeze_(-1)\n",
    "    hidden = rnn.getInitialisedHidden()\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for input_letter_tensor, target_letter_tensor in zip(input_word_tensor, target_word_tensor):\n",
    "        #output, hidden = rnn(input_letter_tensor, hidden)\n",
    "        output, hidden = rnn(input_letter_tensor, hidden)\n",
    "        # Add up losses for each letter prediction\n",
    "        loss += criterion(output, target_letter_tensor)\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    for parameter in rnn.parameters():\n",
    "        parameter.data.add_(parameter.grad.data, alpha=-learning_rate)\n",
    "        \n",
    "    return output, loss.item() / input_word_tensor.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_rnn = firstRNN(LETTERS_TOTAL, 128, LETTERS_TOTAL)\n",
    "\n",
    "hidden = exp_rnn.getInitialisedHidden()\n",
    "inp, out = getRandomTraningTensorSet(words)\n",
    "first_letter = inp[0]\n",
    "output, hidden = exp_rnn(first_letter, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.2770, -3.7179, -3.7710, -4.1199, -4.1054, -4.1442, -3.8545, -3.9437,\n",
       "         -3.9825, -3.4763, -4.4841, -3.9566, -3.8151, -4.5057, -4.0466, -3.6666,\n",
       "         -4.3094, -4.4502, -3.7508, -3.4739, -3.9692, -4.1555, -4.0466, -3.5082,\n",
       "         -4.0984, -3.8538, -3.9955, -4.4138, -4.4183, -4.3660, -3.6728, -4.1251,\n",
       "         -4.5279, -4.1281, -3.9539, -3.8355, -3.8203, -3.8174, -4.0334, -4.0459,\n",
       "         -4.0466, -4.2116, -4.0663, -4.0448, -4.1923, -4.0337, -4.0630, -3.3131,\n",
       "         -4.5195, -3.5055, -4.4965, -4.3511, -4.1000, -4.1606]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s/60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = firstRNN(LETTERS_TOTAL, 128, LETTERS_TOTAL)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 10\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_tensor, target_tensor = getRandomTraningTensorSet(words)\n",
    "    output, loss = train(input_tensor, target_tensor, rnn)\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstRNN(\n",
       "  (i2h): Linear(in_features=182, out_features=128, bias=True)\n",
       "  (i2o): Linear(in_features=182, out_features=54, bias=True)\n",
       "  (o2o): Linear(in_features=182, out_features=54, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_letter = 'h'\n",
    "\n",
    "def evaluate(input_letter, rnn):\n",
    "    '''Gimme me a word'''\n",
    "    with torch.no_grad():\n",
    "        letters = [input_letter]\n",
    "\n",
    "        input_letter_tensor = inputTensor(input_letter)\n",
    "        hidden = rnn.getInitialisedHidden()\n",
    "        \n",
    "        counter = 0\n",
    "        while(counter < 20):\n",
    "            output, hidden = rnn(input_letter_tensor[0], hidden)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == LETTERS_TOTAL - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = ALL_LETTERS[topi]\n",
    "                letters.append(letter)\n",
    "                input_letter_tensor = inputTensor(letter)\n",
    "            counter += 1\n",
    "        \n",
    "        return ''.join(letters)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yyyyyyyyyiiiiiiiihhhh'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('y', backup_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's some gibberish. Let's train it with words that contain fewer letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_letter_words = [word for word in words if len(word) <=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 16s (5000 5%) 3.0619\n",
      "0m 32s (10000 10%) 3.7339\n",
      "0m 48s (15000 15%) 4.8852\n",
      "1m 2s (20000 20%) 2.5823\n",
      "1m 16s (25000 25%) 0.5806\n",
      "1m 33s (30000 30%) 2.6278\n",
      "1m 51s (35000 35%) 0.5961\n",
      "2m 8s (40000 40%) 3.2147\n",
      "2m 23s (45000 45%) 0.4084\n",
      "2m 39s (50000 50%) 1.2613\n",
      "2m 59s (55000 55%) 0.2334\n",
      "3m 17s (60000 60%) 1.8040\n",
      "3m 35s (65000 65%) 1.1736\n",
      "3m 54s (70000 70%) 1.2111\n",
      "4m 8s (75000 75%) 1.8771\n",
      "4m 28s (80000 80%) 0.0334\n",
      "4m 48s (85000 85%) 0.8815\n",
      "5m 7s (90000 90%) 0.0637\n",
      "5m 23s (95000 95%) 0.0268\n",
      "5m 43s (100000 100%) 0.1263\n"
     ]
    }
   ],
   "source": [
    "rnn = firstRNN(LETTERS_TOTAL, 128, LETTERS_TOTAL)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "four_letter_words = [word for word in words if len(word) <=3]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_tensor, target_tensor = getRandomTraningTensorSet(four_letter_words)\n",
    "    output, loss = train(input_tensor, target_tensor, rnn)\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccannyynyyyyyyByyyyyy\n",
      "ccannyzyyyyzyyyyyyyAy\n",
      "ccannyyyyyyyyyyyyyjyy\n",
      "ccannnyyyyyywyyyFyyyy\n",
      "cpanyyyyyymyyyyyyyiyy\n",
      "ccannyyyyyyyyyyyWyyty\n",
      "ccannyGydyyyyyyyyyyyN\n",
      "ccannyyyyyyyUyyyyyyyy\n",
      "ccannyyyyyyyyyyyyyyyy\n",
      "ccannyyyyyyyyyyyyyYyy\n",
      "ccannyryyyyyyyyyyyyyy\n",
      "ccannyyyyyyyyyywyyyyy\n",
      "ccannyyyyyhyyynyyyyyy\n",
      "ccanayyyyyyyyyyjyyyyy\n",
      "ccannyyyyyyyyyyGCyyyy\n",
      "ccanyyyyyyyyyyyyyQyyy\n",
      "ccannyyyyyyyyyyyyyyyy\n",
      "cchnyyyyyyyymyyyyyyyy\n",
      "ccannyynyyyyyyyyZyyyy\n",
      "ccannyyyyyjAyyyyyyyyy\n",
      "ccannyyyyyyyyyyyyyyyy\n",
      "ccJnynnyyyyyyyyyyyyoy\n",
      "ccannyyyyyyysyyyykyyy\n",
      "ccaYnnyyyyMyyyyyjyyyy\n",
      "ccannyyyySyyyyyyjyyyy\n",
      "ccannyyyyyyyyyyyyxyyy\n",
      "cvanyyyyUyyyyyyyyyyyy\n",
      "ccannyqyyyyyyyyymyygy\n",
      "ccannyyyvyyyyyyyyyyyy\n",
      "ccanyyyyyyyyyyyyyyyyy\n",
      "cuanyyyyyyyyyyjyyyyyy\n",
      "ccanayyyyyyynyyyyyyyy\n",
      "ccannnnyyyyyyyyyyyyyy\n",
      "ccannyyyyyyyysyyymyyy\n",
      "ccannyyyyyynyyyywyyyy\n",
      "ccannyyyyyyyyyyyyyyyy\n",
      "ccannyyyyyyyyyyyyyyyx\n",
      "cTanyyyyyyyyyyMyysyny\n",
      "ccannyyyyyyyyOyyyyzyy\n",
      "ccaqnyyyyyyyyyywyyyyA\n",
      "cpanyyIyyyyyyyyyyyyyy\n",
      "ccannnnyyygyyyyyyyyyy\n",
      "ccannyyyyyyyyyyyyyyyy\n",
      "ccannyynyyyyyyylyyyyy\n",
      "ccannyynyyyytyyWyyyyy\n",
      "ccaMyyyyyyyyyyyyyyyyy\n",
      "ccannyyyyyyyykyyyyyyy\n",
      "ccanyyyyyymyyyyyyyyyy\n",
      "ccanyyyyyyyyyyyyyyyyy\n",
      "ccznyyyAyyyyyyyyyyyyy\n"
     ]
    }
   ],
   "source": [
    "for pred in [evaluate('c', rnn) for i in range(50)]:\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class secondRNN(nn.Module):\n",
    "    '''My second ever Recurrent Neural Network\n",
    "    \n",
    "    This one's has got a few more layers in hope that it will be much more capable\n",
    "    to predict closely correlated letters.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Top-level layers\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        \n",
    "        # Middle-stack layers\n",
    "        self.h2h1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2h2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2h3 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.io2o1 = nn.Linear(output_size, output_size)\n",
    "        self.io2o2 = nn.Linear(output_size, output_size)\n",
    "        self.io2o3 = nn.Linear(output_size, output_size)\n",
    "        \n",
    "        # bottom-level layer\n",
    "        self.o2o = nn.Linear(output_size + hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        '''Forward function for training'''\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        \n",
    "        hidden = self.h2h1(hidden)\n",
    "        output = self.io2o1(output)\n",
    "        hidden = self.h2h2(hidden)\n",
    "        output = self.io2o2(output)\n",
    "        hidden = self.h2h3(hidden)\n",
    "        output = self.io2o3(output)\n",
    "        \n",
    "        combined_output = torch.cat((output, hidden), 1)\n",
    "        output = self.o2o(combined_output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "        \n",
    "    def getInitialisedHidden(self):\n",
    "        return torch.ones([1, self.hidden_size])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 43s (5000 5%) 1.4400\n",
      "1m 26s (10000 10%) 2.3192\n",
      "2m 9s (15000 15%) 0.0271\n",
      "2m 54s (20000 20%) 0.0127\n",
      "3m 43s (25000 25%) nan\n",
      "4m 24s (30000 30%) nan\n",
      "5m 6s (35000 35%) nan\n",
      "5m 51s (40000 40%) nan\n",
      "6m 38s (45000 45%) nan\n",
      "7m 21s (50000 50%) nan\n",
      "8m 1s (55000 55%) nan\n",
      "8m 40s (60000 60%) nan\n",
      "9m 21s (65000 65%) nan\n",
      "9m 55s (70000 70%) nan\n",
      "10m 30s (75000 75%) nan\n",
      "11m 9s (80000 80%) nan\n",
      "11m 50s (85000 85%) nan\n",
      "12m 37s (90000 90%) nan\n",
      "13m 21s (95000 95%) nan\n",
      "13m 59s (100000 100%) nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn = secondRNN(LETTERS_TOTAL, 128, LETTERS_TOTAL)\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    input_tensor, target_tensor = getRandomTraningTensorSet(['fuck', 'shit', 'hell', 'hole', 'hill', 'piss', 'poop'])\n",
    "    output, loss = train(input_tensor, target_tensor, rnn)\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fhhoeeeeeeeeeeeeepeep'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('f', rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The thing just keeps crashing with either 3 or 5 layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
